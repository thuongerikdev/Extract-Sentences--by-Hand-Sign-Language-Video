{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7OwIKrs97mv",
        "outputId": "66b1f880-8961-47fe-f2ac-9e7ecb297295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.15 protobuf-4.25.5 sounddevice-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O image.jpg https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/woman_hands.jpg\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n"
      ],
      "metadata": {
        "id": "u6W3XDY3-P3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from mediapipe.tasks.python.components.containers import landmark as landmark_module\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import mediapipe as mp\n",
        "from typing import List, Tuple, Union, Mapping, Optional\n",
        "\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "\n",
        "#  if not results.multi_hand_world_landmarks:\n",
        "#       continue\n",
        "#     for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "#       mp_drawing.plot_landmarks(\n",
        "#         hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n",
        "\n",
        "BaseOptions = mp.tasks.BaseOptions\n",
        "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
        "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
        "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
        "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54)\n",
        "\n",
        "indices_to_keep = [0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 23, 24]\n",
        "#                  0  1  2  3  4  5   6   7   8   9   10  11 12\n",
        "\n",
        "\n",
        "POSE_CONNECTIONS = frozenset([(0,1),(0,2),(1,3),(2,4),(5,6),(6,8),(8,10),\n",
        "                              (5,7),(7,9),(5,11),(6,12),(11,12)])\n",
        "\n",
        "\n",
        "def extract_keypoints_and_bbox(hand_landmarks):\n",
        "    x_min = min([lm.x for lm in hand_landmarks])\n",
        "    y_min = min([lm.y for lm in hand_landmarks])\n",
        "    x_max = max([lm.x for lm in hand_landmarks])\n",
        "    y_max = max([lm.y for lm in hand_landmarks])\n",
        "\n",
        "    width = x_max - x_min\n",
        "    height = y_max - y_min\n",
        "    bbox = (x_min, y_min, width, height)\n",
        "\n",
        "    keypoints = [(lm.x, lm.y) for lm in hand_landmarks]\n",
        "\n",
        "    return keypoints, bbox\n",
        "\n",
        "def ensure_hands_order(hand_landmarks, handedness):\n",
        "    \"\"\"\n",
        "    Sắp xếp tay sao cho tay phải luôn đứng trước tay trái.\n",
        "    Nếu thiếu tay, thêm tay giả với tất cả tọa độ là 0.\n",
        "    \"\"\"\n",
        "    # Sắp xếp tay theo handedness\n",
        "    best_hands = {'Right': (None, -1), 'Left': (None, -1)}\n",
        "    #print(handedness)\n",
        "    # Iterate through handedness and hand_landmarks together\n",
        "    for idx, (landmarks, hand_info) in enumerate(zip(hand_landmarks, handedness)):\n",
        "        for category in hand_info:\n",
        "            if category.category_name in best_hands:\n",
        "                _, best_score = best_hands[category.category_name]\n",
        "                if category.score > best_score:\n",
        "                    best_hands[category.category_name] = (landmarks, category.score)\n",
        "\n",
        "    # Preparing the ordered landmarks list\n",
        "    ordered_landmarks = []\n",
        "\n",
        "    # Append the best right hand if found, otherwise append an empty hand\n",
        "    if best_hands['Right'][0] is not None:\n",
        "        ordered_landmarks.append(best_hands['Right'][0])\n",
        "    else:\n",
        "        ordered_landmarks.append(create_empty_hand())\n",
        "\n",
        "    # Append the best left hand if found, otherwise append an empty hand\n",
        "    if best_hands['Left'][0] is not None:\n",
        "        ordered_landmarks.append(best_hands['Left'][0])\n",
        "    else:\n",
        "        ordered_landmarks.append(create_empty_hand())\n",
        "\n",
        "    return ordered_landmarks\n",
        "\n",
        "def create_empty_hand():\n",
        "    #Tạo một tay giả với tất cả tọa độ x, y, z là 0\n",
        "    empty_landmarks = [\n",
        "        landmark_module.NormalizedLandmark(x=0, y=0, z=0, visibility=0.0, presence=0.0)\n",
        "        for _ in range(21)\n",
        "    ]\n",
        "    return empty_landmarks\n",
        "\n",
        "\n",
        "def draw_hand_landmarks_on_image(rgb_image, detection_result):\n",
        "  hand_landmarks_list = detection_result\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected hands to visualize.\n",
        "  for idx in range(len(hand_landmarks_list)):\n",
        "    hand_landmarks = hand_landmarks_list[idx]\n",
        "\n",
        "    # Draw the hand landmarks.\n",
        "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    hand_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      hand_landmarks_proto,\n",
        "      solutions.hands.HAND_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "      solutions.drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "  plt.imshow(annotated_image)\n",
        "  plt.show()\n",
        "  return annotated_image\n",
        "\n",
        "\n",
        "def filter_pose_landmarks(pose_detection_result):\n",
        "    # Indices of landmarks to extract\n",
        "    indices_to_extract = [0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 23, 24]\n",
        "\n",
        "    # Check if the pose detection result has landmarks\n",
        "    if not pose_detection_result.pose_landmarks:\n",
        "        return []\n",
        "\n",
        "    # Filter the landmarks based on the specified indices\n",
        "    filtered_landmarks = [\n",
        "        pose_detection_result.pose_landmarks[0][idx]\n",
        "        for idx in indices_to_extract\n",
        "    ]\n",
        "\n",
        "    return filtered_landmarks\n",
        "\n",
        "def draw_pose_landmarks_on_image(rgb_image, detection_result):\n",
        "  pose_landmarks_list = [detection_result]\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected poses to visualize.\n",
        "  for idx in range(len(pose_landmarks_list)):\n",
        "    pose_landmarks = pose_landmarks_list[idx]\n",
        "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    pose_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "    ])\n",
        "    # Draw the pose landmarks.\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      pose_landmarks_proto,\n",
        "      POSE_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
        "  return annotated_image\n",
        "\n",
        "\n",
        "def process(input_path):\n",
        "    is_video = input_path.lower().endswith(('.mp4', '.avi', '.mov'))\n",
        "    results = []\n",
        "    options1 = PoseLandmarkerOptions(\n",
        "            base_options=BaseOptions(model_asset_path='/content/drive/MyDrive/hand_sigh_dataset/pose_landmarker_full.task'),\n",
        "            running_mode=VisionRunningMode.IMAGE, num_poses = 1, min_pose_detection_confidence = 0.5, min_pose_presence_confidence = 0.5)\n",
        "    pose_detector = PoseLandmarker.create_from_options(options1)\n",
        "\n",
        "    if is_video:\n",
        "        # Xử lý video\n",
        "        options = HandLandmarkerOptions(\n",
        "            base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "            running_mode=VisionRunningMode.VIDEO, num_hands=6, min_hand_detection_confidence = 0.1, min_hand_presence_confidence = 0.1, min_tracking_confidence = 0.65)\n",
        "\n",
        "        with HandLandmarker.create_from_options(options) as landmarker:\n",
        "            cap = cv2.VideoCapture(input_path)\n",
        "            frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "            frame_timestamp_ms = 0\n",
        "            a = 0\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame_result = {}\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "                pose_detection_result = pose_detector.detect(mp_image)\n",
        "                if pose_detection_result.pose_landmarks:\n",
        "                    #print(pose_detection_result)\n",
        "                    pose_detection_result = filter_pose_landmarks(pose_detection_result)\n",
        "                    #print(pose_detection_result)\n",
        "                    # Đảm bảo thứ tự tay và xử lý normalize\n",
        "                    result_hand = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
        "                    ordered_hand_landmarks = ensure_hands_order(result_hand.hand_landmarks, result_hand.handedness)\n",
        "                    #print(ordered_hand_landmarks)\n",
        "                    frame_result['hand_landmarks'] = ordered_hand_landmarks\n",
        "                    frame_result['pose_landmarks'] = pose_detection_result\n",
        "                    results.append(frame_result)\n",
        "                    # image = mp_image.numpy_view()\n",
        "                    # annotated_image = draw_pose_landmarks_on_image(image,  pose_detection_result)\n",
        "                    # annotated_image= draw_hand_landmarks_on_image(annotated_image, ordered_hand_landmarks)\n",
        "                    a +=1\n",
        "\n",
        "                frame_timestamp_ms += int(1000 / frame_rate)\n",
        "            print(a)\n",
        "            cap.release()\n",
        "\n",
        "    else:\n",
        "        # Xử lý ảnh\n",
        "        options = HandLandmarkerOptions(\n",
        "            base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "            running_mode=VisionRunningMode.IMAGE,num_hands=6, min_hand_detection_confidence = 0.1, min_hand_presence_confidence = 0.1, min_tracking_confidence = 0.65 )\n",
        "        with HandLandmarker.create_from_options(options) as landmarker:\n",
        "            mp_image = mp.Image.create_from_file(input_path)\n",
        "            #mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb) # Đọc từ một numpy array ảnh\n",
        "\n",
        "            result = landmarker.detect(mp_image)\n",
        "            print(result)\n",
        "            pose_detection_result = pose_detector.detect(mp_image)\n",
        "            print(pose_detection_result)\n",
        "            frame_result = {}\n",
        "            #print(result)\n",
        "\n",
        "            handedness_categories = {category.category_name for hand_info in result.handedness for category in hand_info}\n",
        "            if 'Left' in handedness_categories and 'Right' in handedness_categories:\n",
        "                ordered_hand_landmarks = ensure_hands_order(result.hand_landmarks, result.handedness)\n",
        "                frame_result['hand_landmarks'] = ordered_hand_landmarks\n",
        "                results.append(frame_result)\n",
        "                image = mp_image.numpy_view()\n",
        "                image = image[:, :, :3]\n",
        "                if pose_detection_result.pose_landmarks:\n",
        "                    pose_detection_result = filter_pose_landmarks(pose_detection_result)\n",
        "                    frame_result['pose_landmarks'] = pose_detection_result\n",
        "                    print(image.shape)\n",
        "                    annotated_image = draw_pose_landmarks_on_image(image,  pose_detection_result)\n",
        "                else:\n",
        "                  annotated_imagee = image\n",
        "                annotated_image= draw_hand_landmarks_on_image(annotated_image, ordered_hand_landmarks)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "sBkYAFUK98rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def normalize_hand_landmarks(hand_landmarks):\n",
        "    # Chia tay trái và phải\n",
        "    left_hand = hand_landmarks[1] if len(hand_landmarks) > 1 else []\n",
        "    right_hand = hand_landmarks[0] if len(hand_landmarks) > 0 else []\n",
        "\n",
        "    def normalize_single_hand(hand):\n",
        "        if not hand:\n",
        "            return np.zeros(21*2)  # Nếu không có tay, trả về array toàn số 0\n",
        "\n",
        "        # Lấy giá trị x và y từ landmarks\n",
        "        landmarks_x_values = [lm.x for lm in hand]\n",
        "        landmarks_y_values = [lm.y for lm in hand]\n",
        "\n",
        "        # Tính toán bounding box dựa trên chiều rộng và chiều cao\n",
        "        width = max(landmarks_x_values) - min(landmarks_x_values)\n",
        "        height = max(landmarks_y_values) - min(landmarks_y_values)\n",
        "\n",
        "        # Thiết lập delta_x và delta_y theo tỉ lệ width và height\n",
        "        if width > height:\n",
        "            delta_x = 0.3 * width\n",
        "            delta_y = delta_x + ((width - height) / 2)*0.2\n",
        "        else:\n",
        "            delta_y = 0.3 * height\n",
        "            delta_x = delta_y + ((height - width) / 2)*0.2\n",
        "\n",
        "\n",
        "        # Điểm bắt đầu và kết thúc của bounding box để chuẩn hóa\n",
        "        starting_point = [min(landmarks_x_values) - delta_x, min(landmarks_y_values) - delta_y]\n",
        "        ending_point = [max(landmarks_x_values) + delta_x, max(landmarks_y_values) + delta_y]\n",
        "\n",
        "        # Chuẩn hóa các điểm landmarks\n",
        "        normalized_landmarks = []\n",
        "        for lm in hand:\n",
        "            if (ending_point[0] - starting_point[0]) == 0 or (starting_point[1] - ending_point[1]) == 0:\n",
        "                normalized_landmarks.append((0.0,0.0))\n",
        "                continue\n",
        "            normalized_x = (lm.x - starting_point[0]) / (ending_point[0] - starting_point[0])\n",
        "            normalized_y = (lm.y - ending_point[1]) / (starting_point[1] - ending_point[1])\n",
        "            normalized_landmarks.append((normalized_x - 0.5, normalized_y - 0.5))\n",
        "\n",
        "        return np.array(normalized_landmarks).flatten(), [starting_point, ending_point]\n",
        "\n",
        "\n",
        "    # Chuẩn hóa tay trái và phải\n",
        "    left_normalized, point1 = normalize_single_hand(left_hand)\n",
        "    right_normalized, point2 = normalize_single_hand(right_hand)\n",
        "\n",
        "    # Trả về kết quả đã chuẩn hóa\n",
        "    return np.concatenate([right_normalized, left_normalized]), [point1, point2]\n",
        "    #return [right_normalized, left_normalized], [point1, point2]\n",
        "\n",
        "def normalize_pose_landmarks(pose_landmarks):\n",
        "\n",
        "    landmarks_x_values = [lm.x for lm in pose_landmarks]\n",
        "    landmarks_y_values = [lm.y for lm in pose_landmarks]\n",
        "\n",
        "    # Lấy vị trí của vai trái (left_shoulder) và vai phải (right_shoulder)\n",
        "    left_shoulder = (pose_landmarks[5].x, pose_landmarks[5].y)\n",
        "    right_shoulder = (pose_landmarks[6].x, pose_landmarks[6].y)\n",
        "\n",
        "    # Tính khoảng cách giữa vai trái và vai phải\n",
        "    shoulder_distance = (((left_shoulder[0] - right_shoulder[0]) ** 2) +\n",
        "                         ((left_shoulder[1] - right_shoulder[1]) ** 2)) ** 0.5\n",
        "    head_metric = shoulder_distance\n",
        "\n",
        "    # Tính toán starting_point và ending_point dựa trên thông tin neck và mắt trái\n",
        "    neck_x = (pose_landmarks[5].x + pose_landmarks[6].x) / 2  # Giả sử cổ nằm giữa hai vai\n",
        "    #print(\"neck\", neck_x)\n",
        "    left_eye_y = pose_landmarks[1].y\n",
        "\n",
        "\n",
        "    starting_point = [neck_x - 3 * head_metric, left_eye_y - (head_metric)]\n",
        "    ending_point = [neck_x + 3 * head_metric , starting_point[1] +  6 * head_metric]\n",
        "  #print([starting_point[0], ending_point[0]])\n",
        "\n",
        "    # Chuẩn hóa các điểm landmarks\n",
        "    normalized_landmarks = []\n",
        "    for lm in pose_landmarks:\n",
        "        normalized_x = (lm.x - starting_point[0]) / (ending_point[0] - starting_point[0])\n",
        "        normalized_y = (lm.y - ending_point[1]) / (starting_point[1] - ending_point[1])\n",
        "        normalized_landmarks.append((normalized_x - 0.5, normalized_y - 0.5))\n",
        "\n",
        "    return np.array(normalized_landmarks).flatten(), [starting_point, ending_point]\n",
        "\n",
        "def normalize_landmarks(results):\n",
        "    all_frames_normalized = []\n",
        "\n",
        "    for frame_result in results:\n",
        "        hand_normalized,a = normalize_hand_landmarks(frame_result['hand_landmarks'])\n",
        "        pose_normalized,b = normalize_pose_landmarks(frame_result['pose_landmarks'])\n",
        "\n",
        "        frame_normalized = np.concatenate([hand_normalized, pose_normalized])\n",
        "        all_frames_normalized.append(frame_normalized)\n",
        "\n",
        "    return np.array(all_frames_normalized)"
      ],
      "metadata": {
        "id": "8zMu9Nz2-Isw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_or_trim_landmarks(landmarks, target_length=200):\n",
        "\n",
        "    current_length = landmarks.shape[0]\n",
        "\n",
        "    if current_length < target_length:\n",
        "        # Padding: Add zeros to the end\n",
        "        pad_length = target_length - current_length\n",
        "        padding = np.zeros((pad_length, landmarks.shape[1]))\n",
        "        padded_landmarks = np.vstack((landmarks, padding))\n",
        "        return padded_landmarks\n",
        "    else:\n",
        "        # Trimming: Slice the array to the target length\n",
        "        return landmarks[:target_length, :]"
      ],
      "metadata": {
        "id": "tZlogYnc-b6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class GaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ],
      "metadata": {
        "id": "YYuEhDL3tG1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device, scheduler=None):\n",
        "\n",
        "    pred_correct, pred_all = 0, 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data[0], data[1]\n",
        "        inputs = inputs.squeeze(0).to(device, non_blocking=True)\n",
        "        labels = labels.to(device, dtype=torch.long, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).expand(1, -1, -1)\n",
        "        # print(outputs[0].shape)\n",
        "        # print(labels[0].shape)\n",
        "        loss = criterion(outputs[0], labels[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss\n",
        "\n",
        "        # Statistics\n",
        "        if int(torch.argmax(torch.nn.functional.softmax(outputs, dim=2))) == int(labels[0][0]):\n",
        "            pred_correct += 1\n",
        "        pred_all += 1\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step(running_loss.item() / len(dataloader))\n",
        "\n",
        "    return running_loss, pred_correct, pred_all, (pred_correct / pred_all)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device, print_stats=False):\n",
        "\n",
        "    pred_correct, pred_all = 0, 0\n",
        "    stats = {i: [0, 0] for i in range(250)}\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data[0], data[1]\n",
        "        inputs = inputs.squeeze(0).to(device, non_blocking=True)\n",
        "        labels = labels.to(device, dtype=torch.long, non_blocking=True)\n",
        "\n",
        "        outputs = model(inputs).expand(1, -1, -1)\n",
        "\n",
        "        # Statistics\n",
        "        if int(torch.argmax(torch.nn.functional.softmax(outputs, dim=2))) == int(labels[0][0]):\n",
        "            stats[int(labels[0][0])][0] += 1\n",
        "            pred_correct += 1\n",
        "\n",
        "        stats[int(labels[0][0])][1] += 1\n",
        "        pred_all += 1\n",
        "\n",
        "    if print_stats:\n",
        "        stats_1 = {key: value[0] / value[1] for key, value in stats.items() if value[1] != 0}\n",
        "        print(\"Label accuracies statistics:\")\n",
        "        print(str(stats_1) + \"\\n\")\n",
        "        stats_test = {key: value[0] for key, value in stats.items() if value[1] != 0 }\n",
        "        stats_real = {key: value[1] for key, value in stats.items() if value[1] != 0 }\n",
        "        print(str(stats_test) + \"\\n\")\n",
        "        print(str(stats_real) + \"\\n\")\n",
        "        logging.info(\"Label accuracies statistics:\")\n",
        "        logging.info(str(stats) + \"\\n\")\n",
        "\n",
        "    return pred_correct, pred_all, (pred_correct / pred_all)\n",
        "\n",
        "\n",
        "def evaluate_top_k(model, dataloader, device, k=5):\n",
        "\n",
        "    pred_correct, pred_all = 0, 0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.squeeze(0).to(device)\n",
        "        labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(inputs).expand(1, -1, -1)\n",
        "\n",
        "        if int(labels[0][0]) in torch.topk(outputs, k).indices.tolist():\n",
        "            pred_correct += 1\n",
        "\n",
        "        pred_all += 1\n",
        "\n",
        "    return pred_correct, pred_all, (pred_correct / pred_all)"
      ],
      "metadata": {
        "id": "moRkR366tJYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTxo0R4vrXx3"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "def _get_clones(mod, n):\n",
        "    return nn.ModuleList([copy.deepcopy(mod) for _ in range(n)])\n",
        "\n",
        "class DummySelfAttention(nn.Module):\n",
        "    def __init__(self, batch_first=True):\n",
        "        super(DummySelfAttention, self).__init__()\n",
        "        # Keep the required attributes\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        # Just return the input as-is\n",
        "        return args[0]\n",
        "class SPOTERTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
        "    \"\"\"\n",
        "    Edited TransformerDecoderLayer implementation omitting the redundant self-attention operation as opposed to the\n",
        "    standard implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
        "        super(SPOTERTransformerDecoderLayer, self).__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
        "\n",
        "        self.self_attn = DummySelfAttention()\n",
        "\n",
        "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None,\n",
        "                memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[torch.Tensor] = None,\n",
        "                tgt_is_causal=None,\n",
        "                memory_is_causal=None) -> torch.Tensor:\n",
        "\n",
        "        tgt = tgt + self.dropout1(tgt)\n",
        "        tgt = self.norm1(tgt)\n",
        "        #print('memory.shape',memory.shape)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        #print('tgt2.shape',tgt2.shape)\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class SPOTER(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the SPOTER (Sign POse-based TransformER) architecture for sign language recognition from sequence\n",
        "    of skeletal data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, hidden_dim=110):\n",
        "        super().__init__()\n",
        "\n",
        "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim))\n",
        "        self.pos = nn.Parameter(torch.cat([self.row_embed[0].unsqueeze(0).repeat(1, 1, 1)], dim=-1).flatten(0, 1).unsqueeze(0))\n",
        "        self.class_query = nn.Parameter(torch.rand(1, hidden_dim))\n",
        "        self.transformer = nn.Transformer(hidden_dim, 10, 6, 6)\n",
        "        # Deactivate the initial attention decoder mechanism\n",
        "        custom_decoder_layer = SPOTERTransformerDecoderLayer(self.transformer.d_model, self.transformer.nhead, 2048,\n",
        "                                                             0.1, \"relu\")\n",
        "        self.transformer.decoder.layers = _get_clones(custom_decoder_layer, self.transformer.decoder.num_layers)\n",
        "        self.linear_class = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #print('input.shape',inputs.shape)\n",
        "        h = torch.unsqueeze(inputs.flatten(start_dim=1), 1).float()\n",
        "        #print('h.shape',h.shape)\n",
        "        #print('class_query.shape',self.class_query.shape)\n",
        "        #print('class_query.shape.unsqueeze(0)',self.class_query.unsqueeze(0).shape)\n",
        "        h = self.transformer(self.pos + h, self.class_query.unsqueeze(0)).transpose(0, 1)\n",
        "        #print('h2.shape',h.shape)\n",
        "        res = self.linear_class(h)\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Function to read HDF5 file (provided)\n",
        "def read_hdf5_file(file_path):\n",
        "    with h5py.File(file_path, 'r') as hf:\n",
        "        data = {}\n",
        "        for key in hf.keys():\n",
        "            data[key] = hf[key][:]  # Read datasets into numpy arrays\n",
        "        return data\n",
        "\n",
        "# Custom Dataset class for loading data from HDF5\n",
        "class HDF5Dataset(Dataset):\n",
        "    def __init__(self, hdf5_file_path):\n",
        "        # Load the data from the HDF5 file\n",
        "        self.data = read_hdf5_file(hdf5_file_path)\n",
        "\n",
        "        # Extract the results, labels, and label_encodes\n",
        "        self.results = self.data['results']\n",
        "        self.num_labels = self.data['num_label']\n",
        "        self.label_encodes = self.data.get('label_encode', None)  # Handle case where label_encode may not exist\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.num_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load a single data point\n",
        "        results = self.results[idx]\n",
        "\n",
        "        # Get the label and label_encode if available\n",
        "        num_label = self.num_labels[idx]\n",
        "        label_encode = self.label_encodes[idx] if self.label_encodes is not None else None\n",
        "\n",
        "        # Convert to torch tensor for PyTorch compatibility\n",
        "        results = torch.tensor(results, dtype=torch.float32)\n",
        "\n",
        "        num_label = torch.tensor([num_label], dtype=torch.long)\n",
        "        if label_encode is not None:\n",
        "          label_encode = torch.tensor(label_encode, dtype=torch.long)\n",
        "\n",
        "        # Return a tuple of (results, label, label_encode)\n",
        "\n",
        "        return results, num_label\n"
      ],
      "metadata": {
        "id": "g8FmMfcymEcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "def __balance_val_split(dataset, val_split=0.):\n",
        "    # Lấy nhãn trực tiếp từ dataset\n",
        "    labels = dataset.num_labels\n",
        "    print(len(list(labels)))\n",
        "\n",
        "    # Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        np.arange(len(labels)),  # Sử dụng len(labels) để có số lượng mẫu\n",
        "        test_size=val_split,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(dataset, indices=train_indices)\n",
        "    val_dataset = Subset(dataset, indices=val_indices)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def __split_of_train_sequence(subset: Subset, train_split=1.0):\n",
        "    if train_split == 1:\n",
        "        return subset\n",
        "\n",
        "    targets = np.array([subset.dataset.targets[i] for i in subset.indices])\n",
        "    train_indices, _ = train_test_split(\n",
        "        np.arange(targets.shape[0]),\n",
        "        test_size=1 - train_split,\n",
        "        stratify=targets\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(subset.dataset, indices=[subset.indices[i] for i in train_indices])\n",
        "\n",
        "    return train_dataset\n",
        "\n",
        "\n",
        "def __log_class_statistics(subset: Subset):\n",
        "    train_classes = [subset.dataset.targets[i] for i in subset.indices]\n",
        "    print(dict(Counter(train_classes)))"
      ],
      "metadata": {
        "id": "8e_C2jD5TBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import logging\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "def get_default_args():\n",
        "    parser = argparse.ArgumentParser(add_help=False)\n",
        "\n",
        "    parser.add_argument(\"--experiment_name\", type=str, default=\"lsa_64_spoter\",\n",
        "                        help=\"Name of the experiment after which the logs and plots will be named\")\n",
        "    parser.add_argument(\"--num_classes\", type=int, default=400, help=\"Number of classes to be recognized by the model\")\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=110,\n",
        "                        help=\"Hidden dimension of the underlying Transformer model\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=379,\n",
        "                        help=\"Seed with which to initialize all the random components of the training\")\n",
        "\n",
        "     # Data\n",
        "    parser.add_argument(\"--training_set_path\", type=str, default=\"/content/drive/MyDrive/hand_sigh_dataset/h5/train_datatuquay100.h5\", help=\"Path to the training dataset h5 file\")\n",
        "    parser.add_argument(\"--testing_set_path\", type=str, default=\"/content/drive/MyDrive/hand_sigh_dataset/h5/test_datatuquay100.h5\", help=\"Path to the testing dataset h5 file\")\n",
        "    parser.add_argument(\"--experimental_train_split\", type=float, default=1.0,\n",
        "                        help=\"Determines how big a portion of the training set should be employed (intended for the \"\n",
        "                             \"gradually enlarging training set experiment from the paper)\")\n",
        "\n",
        "    parser.add_argument(\"--validation_set\", type=str, choices=[\"from-file\", \"split-from-train\", \"none\"],\n",
        "                        default=\"split-from-train\", help=\"Type of validation set construction. See README for further rederence\")\n",
        "    parser.add_argument(\"--validation_set_path\", type=str, default=\"\", help=\"Path to the validation dataset CSV file\")\n",
        "\n",
        "    # Training hyperparameters\n",
        "    parser.add_argument(\"--epochs\", type=int, default=11, help=\"Number of epochs to train the model for\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"Learning rate for the model training\")\n",
        "    parser.add_argument(\"--log_freq\", type=int, default=1,\n",
        "                        help=\"Log frequency (frequency of printing all the training info\")\n",
        "\n",
        "    #Checkpointing\n",
        "    parser.add_argument(\"--save_checkpoints\", type=bool, default=True,\n",
        "                        help=\"Determines whether to save weights checkpoints\")\n",
        "    parser.add_argument(\"--checkpoints_path\", type=str, default=\"/content/drive/MyDrive/hand_sigh_dataset/ckpt/ckpt_110/checkpoint_100_2_nopretrain.pth\",\n",
        "                        help=\"Path to cpkt\")\n",
        "\n",
        "    # Scheduler\n",
        "    parser.add_argument(\"--scheduler_factor\", type=int, default=0.1, help=\"Factor for the ReduceLROnPlateau scheduler\")\n",
        "    parser.add_argument(\"--scheduler_patience\", type=int, default=5,\n",
        "                        help=\"Patience for the ReduceLROnPlateau scheduler\")\n",
        "\n",
        "    # Gaussian noise normalization\n",
        "    parser.add_argument(\"--gaussian_mean\", type=int, default=0, help=\"Mean parameter for Gaussian noise layer\")\n",
        "    parser.add_argument(\"--gaussian_std\", type=int, default=0.001,\n",
        "                        help=\"Standard deviation parameter for Gaussian noise layer\")\n",
        "\n",
        "    # Visualization\n",
        "    parser.add_argument(\"--plot_stats\", type=bool, default=True,\n",
        "                        help=\"Determines whether continuous statistics should be plotted at the end\")\n",
        "    parser.add_argument(\"--plot_lr\", type=bool, default=True,\n",
        "                        help=\"Determines whether the LR should be plotted at the end\")\n",
        "    parser.add_argument(\"-f\", required=False)\n",
        "    return parser\n",
        "\n"
      ],
      "metadata": {
        "id": "6xB76m18T_oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6b0L6wxyeED",
        "outputId": "e030f9af-f175-4ad0-808c-4f86f6d4aa0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "\n",
        "    # MARK: TRAINING PREPARATION AND MODULES\n",
        "\n",
        "    # Initialize all the random seeds\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(args.seed)\n",
        "\n",
        "    # Set the output format to print into the console and save into LOG file\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "        handlers=[\n",
        "            logging.FileHandler(args.experiment_name + \"_\" + str(args.experimental_train_split).replace(\".\", \"\") + \".log\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Set device to CUDA only if applicable\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Construct the model\n",
        "    slrt_model = SPOTER(num_classes=args.num_classes, hidden_dim=args.hidden_dim)\n",
        "    if args.checkpoints_path:\n",
        "        if not torch.cuda.is_available():\n",
        "          checkpoint = torch.load(args.checkpoints_path, map_location=torch.device('cpu'))\n",
        "        else:\n",
        "          checkpoint = torch.load(args.checkpoints_path)\n",
        "        full_state_dict = checkpoint.state_dict()\n",
        "        #new_state_dict = {k: v for k, v in full_state_dict.items() if 'linear_class' not in k}\n",
        "        slrt_model.load_state_dict(full_state_dict, strict=False)\n",
        "    slrt_model.train(True)\n",
        "    slrt_model.to(device)\n",
        "\n",
        "    # Construct the other modules\n",
        "    cel_criterion = nn.CrossEntropyLoss().to(device)\n",
        "    sgd_optimizer = optim.SGD(slrt_model.parameters(), lr=args.lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(sgd_optimizer, factor=args.scheduler_factor, patience=args.scheduler_patience)\n",
        "\n",
        "    # Ensure that the path for checkpointing and for images both exist\n",
        "    Path(\"out-checkpoints/\" + args.experiment_name + \"/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(\"out-img/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "    # MARK: DATA\n",
        "\n",
        "    # Training set\n",
        "    transform = transforms.Compose([GaussianNoise(args.gaussian_mean, args.gaussian_std)])\n",
        "    train_set = HDF5Dataset(args.training_set_path)\n",
        "\n",
        "    # Validation set\n",
        "    if args.validation_set == \"from-file\":\n",
        "        val_set = HDF5Dataset(args.validation_set_path)\n",
        "        val_loader = DataLoader(val_set, shuffle=True, generator=g, pin_memory=True if torch.cuda.is_available() else False)\n",
        "\n",
        "    elif args.validation_set == \"split-from-train\":\n",
        "        train_set, val_set = __balance_val_split(train_set, 0.2)\n",
        "        val_loader = DataLoader(val_set, shuffle=True, generator=g, pin_memory=True if torch.cuda.is_available() else False)\n",
        "\n",
        "    else:\n",
        "        val_loader = None\n",
        "\n",
        "    # Testing set\n",
        "    if args.testing_set_path:\n",
        "        eval_set = HDF5Dataset(args.testing_set_path)\n",
        "        eval_loader = DataLoader(eval_set, shuffle=True, generator=g, pin_memory=True if torch.cuda.is_available() else False)\n",
        "\n",
        "    else:\n",
        "        eval_loader = None\n",
        "\n",
        "    # Final training set refinements\n",
        "    if args.experimental_train_split:\n",
        "        train_set = __split_of_train_sequence(train_set, args.experimental_train_split)\n",
        "\n",
        "    train_loader = DataLoader(train_set, shuffle=True, generator=g, pin_memory=True if torch.cuda.is_available() else False)\n",
        "\n",
        "    #---------------------------------------------------------------------------------------------------------------------\n",
        "    # MARK: TRAINING\n",
        "    train_acc, val_acc = 0, 0\n",
        "    losses, train_accs, val_accs = [], [], []\n",
        "    lr_progress = []\n",
        "    top_train_acc, top_val_acc = 0, 0\n",
        "    checkpoint_index = 0\n",
        "\n",
        "    if args.experimental_train_split:\n",
        "        print(\"Starting \" + args.experiment_name + \"_\" + str(args.experimental_train_split).replace(\".\", \"\") + \"...\\n\\n\")\n",
        "        logging.info(\"Starting \" + args.experiment_name + \"_\" + str(args.experimental_train_split).replace(\".\", \"\") + \"...\\n\\n\")\n",
        "\n",
        "    else:\n",
        "        print(\"Starting \" + args.experiment_name + \"...\\n\\n\")\n",
        "        logging.info(\"Starting \" + args.experiment_name + \"...\\n\\n\")\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss, _, _, train_acc = train_epoch(slrt_model, train_loader, cel_criterion, sgd_optimizer, device)\n",
        "        losses.append(train_loss.item() / len(train_loader))\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        if val_loader:\n",
        "            slrt_model.train(False)\n",
        "            _, _, val_acc = evaluate(slrt_model, val_loader, device)\n",
        "            slrt_model.train(True)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "        # Save checkpoints if they are best in the current subset\n",
        "        if args.save_checkpoints:\n",
        "            if train_acc > top_train_acc:\n",
        "                top_train_acc = train_acc\n",
        "                torch.save(slrt_model, \"out-checkpoints/\" + args.experiment_name + \"/checkpoint_t_\" + str(checkpoint_index) + \".pth\")\n",
        "\n",
        "            if val_acc > top_val_acc:\n",
        "                top_val_acc = val_acc\n",
        "                torch.save(slrt_model, \"out-checkpoints/\" + args.experiment_name + \"/checkpoint_v_\" + str(checkpoint_index) + \".pth\")\n",
        "\n",
        "        if epoch % args.log_freq == 0:\n",
        "            print(\"[\" + str(epoch + 1) + \"] TRAIN  loss: \" + str(train_loss.item() / len(train_loader)) + \" acc: \" + str(train_acc))\n",
        "            logging.info(\"[\" + str(epoch + 1) + \"] TRAIN  loss: \" + str(train_loss.item() / len(train_loader)) + \" acc: \" + str(train_acc))\n",
        "\n",
        "            if val_loader:\n",
        "                print(\"[\" + str(epoch + 1) + \"] VALIDATION  acc: \" + str(val_acc))\n",
        "                logging.info(\"[\" + str(epoch + 1) + \"] VALIDATION  acc: \" + str(val_acc))\n",
        "\n",
        "            print(\"\")\n",
        "            logging.info(\"\")\n",
        "\n",
        "        # Reset the top accuracies on static subsets\n",
        "        if epoch % 5 == 0:\n",
        "            top_train_acc, top_val_acc = 0, 0\n",
        "            checkpoint_index += 1\n",
        "\n",
        "        lr_progress.append(sgd_optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "\n",
        "\n",
        "    # MARK: TESTING\n",
        "\n",
        "    print(\"\\nTesting checkpointed models starting...\\n\")\n",
        "    logging.info(\"\\nTesting checkpointed models starting...\\n\")\n",
        "\n",
        "    top_result, top_result_name = 0, \"\"\n",
        "\n",
        "    if eval_loader:\n",
        "        for i in range(checkpoint_index):\n",
        "            for checkpoint_id in [\"t\", \"v\"]:\n",
        "                # tested_model = VisionTransformer(dim=2, mlp_dim=108, num_classes=100, depth=12, heads=8)\n",
        "                tested_model = torch.load(\"out-checkpoints/\" + args.experiment_name + \"/checkpoint_\" + checkpoint_id + \"_\" + str(i) + \".pth\")\n",
        "                tested_model.train(False)\n",
        "                _, _, eval_acc = evaluate(tested_model, eval_loader, device, print_stats=True)\n",
        "\n",
        "                if eval_acc > top_result:\n",
        "                    top_result = eval_acc\n",
        "                    top_result_name = args.experiment_name + \"/checkpoint_\" + checkpoint_id + \"_\" + str(i)\n",
        "\n",
        "                print(\"checkpoint_\" + checkpoint_id + \"_\" + str(i) + \"  ->  \" + str(eval_acc))\n",
        "                logging.info(\"checkpoint_\" + checkpoint_id + \"_\" + str(i) + \"  ->  \" + str(eval_acc))\n",
        "\n",
        "        print(\"\\nThe top result was recorded at \" + str(top_result) + \" testing accuracy. The best checkpoint is \" + top_result_name + \".\")\n",
        "        logging.info(\"\\nThe top result was recorded at \" + str(top_result) + \" testing accuracy. The best checkpoint is \" + top_result_name + \".\")\n",
        "\n",
        "\n",
        "    #PLOT 0: Performance (loss, accuracies) chart plotting\n",
        "    if args.plot_stats:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(range(1, len(losses) + 1), losses, c=\"#D64436\", label=\"Training loss\")\n",
        "        ax.plot(range(1, len(train_accs) + 1), train_accs, c=\"#00B09B\", label=\"Training accuracy\")\n",
        "\n",
        "        if val_loader:\n",
        "            ax.plot(range(1, len(val_accs) + 1), val_accs, c=\"#E0A938\", label=\"Validation accuracy\")\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
        "\n",
        "        ax.set(xlabel=\"Epoch\", ylabel=\"Accuracy / Loss\", title=\"\")\n",
        "        plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.05), ncol=4, fancybox=True, shadow=True, fontsize=\"xx-small\")\n",
        "        ax.grid()\n",
        "\n",
        "        fig.savefig(\"out-img/\" + args.experiment_name + \"_loss.png\")\n",
        "\n",
        "    # PLOT 1: Learning rate progress\n",
        "    if args.plot_lr:\n",
        "        fig1, ax1 = plt.subplots()\n",
        "        ax1.plot(range(1, len(lr_progress) + 1), lr_progress, label=\"LR\")\n",
        "        ax1.set(xlabel=\"Epoch\", ylabel=\"LR\", title=\"\")\n",
        "        ax1.grid()\n",
        "\n",
        "        fig1.savefig(\"out-img/\" + args.experiment_name + \"_lr.png\")\n",
        "\n",
        "    print(\"\\nAny desired statistics have been plotted.\\nThe experiment is finished.\")\n",
        "    logging.info(\"\\nAny desired statistics have been plotted.\\nThe experiment is finished.\")"
      ],
      "metadata": {
        "id": "eHadHmu_UD3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(\"\", parents=[get_default_args()], add_help=False)\n",
        "\n",
        "args = parser.parse_args()\n",
        "print(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IExPzpyQOyn4",
        "outputId": "646f1ce7-2ef9-43c9-ecf6-953e8d479d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(experiment_name='lsa_64_spoter', num_classes=400, hidden_dim=110, seed=379, training_set_path='/content/drive/MyDrive/hand_sigh_dataset/h5/train_datatuquay100.h5', testing_set_path='/content/drive/MyDrive/hand_sigh_dataset/h5/test_datatuquay100.h5', experimental_train_split=1.0, validation_set='split-from-train', validation_set_path='', epochs=11, lr=0.0001, log_freq=1, save_checkpoints=True, checkpoints_path='/content/drive/MyDrive/hand_sigh_dataset/ckpt/ckpt_110/checkpoint_100_2_nopretrain.pth', scheduler_factor=0.1, scheduler_patience=5, gaussian_mean=0, gaussian_std=0.001, plot_stats=True, plot_lr=True, f='/root/.local/share/jupyter/runtime/kernel-c18f0f7e-f8f1-45c8-b2ac-89bfc2cc57b0.json')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QBOK5CvjiPMg",
        "outputId": "25861ead-e3b8-4616-a642-45d46b4aba37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-9658e80b7d7c>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.checkpoints_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9671\n",
            "Starting lsa_64_spoter_10...\n",
            "\n",
            "\n",
            "[1] TRAIN  loss: 0.05863087130431182 acc: 0.9740175801447777\n",
            "[1] VALIDATION  acc: 0.958656330749354\n",
            "\n",
            "[2] TRAIN  loss: 0.05998668256442081 acc: 0.9709152016546019\n",
            "[2] VALIDATION  acc: 0.9622739018087856\n",
            "\n",
            "[3] TRAIN  loss: 0.05832455150836996 acc: 0.9710444674250258\n",
            "[3] VALIDATION  acc: 0.9643410852713178\n",
            "\n",
            "[4] TRAIN  loss: 0.057388857947017195 acc: 0.9707859358841778\n",
            "[4] VALIDATION  acc: 0.9622739018087856\n",
            "\n",
            "[5] TRAIN  loss: 0.05625885546269067 acc: 0.969622543950362\n",
            "[5] VALIDATION  acc: 0.9612403100775194\n",
            "\n",
            "[6] TRAIN  loss: 0.055462905268388055 acc: 0.9718200620475698\n",
            "[6] VALIDATION  acc: 0.9591731266149871\n",
            "\n",
            "[7] TRAIN  loss: 0.05452101824818624 acc: 0.9741468459152016\n",
            "[7] VALIDATION  acc: 0.9638242894056848\n",
            "\n",
            "[8] TRAIN  loss: 0.05425616306750703 acc: 0.9728541882109617\n",
            "[8] VALIDATION  acc: 0.9596899224806201\n",
            "\n",
            "[9] TRAIN  loss: 0.05411470762329733 acc: 0.9713029989658738\n",
            "[9] VALIDATION  acc: 0.9633074935400516\n",
            "\n",
            "[10] TRAIN  loss: 0.053532794688085736 acc: 0.9729834539813857\n",
            "[10] VALIDATION  acc: 0.958656330749354\n",
            "\n",
            "[11] TRAIN  loss: 0.055562485829997336 acc: 0.9705274043433298\n",
            "[11] VALIDATION  acc: 0.9627906976744186\n",
            "\n",
            "\n",
            "Testing checkpointed models starting...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-9658e80b7d7c>:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tested_model = torch.load(\"out-checkpoints/\" + args.experiment_name + \"/checkpoint_\" + checkpoint_id + \"_\" + str(i) + \".pth\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label accuracies statistics:\n",
            "{0: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 16: 1.0, 17: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 25: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 31: 1.0, 34: 1.0, 36: 1.0, 37: 1.0, 40: 1.0, 41: 1.0, 44: 1.0, 45: 1.0, 48: 1.0, 49: 1.0, 51: 1.0, 52: 1.0, 53: 1.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 0.8, 63: 1.0, 64: 1.0, 65: 0.6, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 75: 1.0, 76: 1.0, 77: 1.0, 78: 0.6, 79: 1.0, 80: 0.8, 81: 1.0, 82: 1.0, 83: 1.0, 84: 1.0, 85: 1.0, 86: 0.0, 87: 0.8, 88: 1.0, 89: 0.4, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 1.0, 98: 0.8, 99: 1.0, 100: 1.0, 102: 1.0, 105: 1.0, 108: 1.0, 109: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 115: 1.0}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 4, 63: 10, 64: 5, 65: 3, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 6, 79: 5, 80: 4, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 0, 87: 8, 88: 5, 89: 2, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 4, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 5, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 10, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 5, 87: 10, 88: 5, 89: 5, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "checkpoint_t_0  ->  0.9606625258799172\n",
            "Label accuracies statistics:\n",
            "{0: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 16: 1.0, 17: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 25: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 31: 1.0, 34: 1.0, 36: 1.0, 37: 1.0, 40: 1.0, 41: 1.0, 44: 1.0, 45: 1.0, 48: 1.0, 49: 1.0, 51: 1.0, 52: 1.0, 53: 1.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 0.8, 63: 1.0, 64: 1.0, 65: 0.6, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 75: 1.0, 76: 1.0, 77: 1.0, 78: 0.6, 79: 1.0, 80: 0.8, 81: 1.0, 82: 1.0, 83: 1.0, 84: 1.0, 85: 1.0, 86: 0.0, 87: 0.8, 88: 1.0, 89: 0.4, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 1.0, 98: 0.8, 99: 1.0, 100: 1.0, 102: 1.0, 105: 1.0, 108: 1.0, 109: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 115: 1.0}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 4, 63: 10, 64: 5, 65: 3, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 6, 79: 5, 80: 4, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 0, 87: 8, 88: 5, 89: 2, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 4, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 5, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 10, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 5, 87: 10, 88: 5, 89: 5, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "checkpoint_v_0  ->  0.9606625258799172\n",
            "Label accuracies statistics:\n",
            "{0: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 16: 1.0, 17: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 25: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 31: 1.0, 34: 1.0, 36: 1.0, 37: 1.0, 40: 1.0, 41: 1.0, 44: 1.0, 45: 1.0, 48: 1.0, 49: 1.0, 51: 1.0, 52: 1.0, 53: 1.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 1.0, 64: 1.0, 65: 0.6, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 75: 1.0, 76: 1.0, 77: 1.0, 78: 0.5, 79: 1.0, 80: 1.0, 81: 1.0, 82: 1.0, 83: 1.0, 84: 1.0, 85: 1.0, 86: 0.0, 87: 1.0, 88: 1.0, 89: 0.0, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 102: 1.0, 105: 1.0, 108: 1.0, 109: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 115: 1.0}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 3, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 5, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 0, 87: 10, 88: 5, 89: 0, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 5, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 10, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 5, 87: 10, 88: 5, 89: 5, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "checkpoint_t_1  ->  0.9648033126293996\n",
            "Label accuracies statistics:\n",
            "{0: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 16: 1.0, 17: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 25: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 31: 1.0, 34: 1.0, 36: 1.0, 37: 1.0, 40: 1.0, 41: 1.0, 44: 1.0, 45: 1.0, 48: 1.0, 49: 1.0, 51: 1.0, 52: 1.0, 53: 1.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 1.0, 64: 1.0, 65: 0.6, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 75: 1.0, 76: 1.0, 77: 1.0, 78: 0.8, 79: 1.0, 80: 0.4, 81: 1.0, 82: 1.0, 83: 1.0, 84: 0.9, 85: 1.0, 86: 0.2, 87: 0.7, 88: 1.0, 89: 0.6, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 102: 1.0, 105: 1.0, 108: 1.0, 109: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 115: 1.0}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 3, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 8, 79: 5, 80: 2, 81: 10, 82: 5, 83: 5, 84: 9, 85: 5, 86: 1, 87: 7, 88: 5, 89: 3, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 5, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 10, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 5, 87: 10, 88: 5, 89: 5, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "checkpoint_v_1  ->  0.9648033126293996\n",
            "Label accuracies statistics:\n",
            "{0: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 16: 1.0, 17: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 25: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 31: 1.0, 34: 1.0, 36: 1.0, 37: 1.0, 40: 1.0, 41: 1.0, 44: 1.0, 45: 1.0, 48: 1.0, 49: 1.0, 51: 1.0, 52: 1.0, 53: 1.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 1.0, 64: 1.0, 65: 0.6, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 75: 1.0, 76: 1.0, 77: 1.0, 78: 0.5, 79: 1.0, 80: 1.0, 81: 1.0, 82: 1.0, 83: 1.0, 84: 0.6, 85: 1.0, 86: 0.8, 87: 0.7, 88: 1.0, 89: 0.6, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 102: 1.0, 105: 1.0, 108: 1.0, 109: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 115: 1.0}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 3, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 5, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 6, 85: 5, 86: 4, 87: 7, 88: 5, 89: 3, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 5, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 10, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 5, 87: 10, 88: 5, 89: 5, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "checkpoint_t_2  ->  0.9648033126293996\n",
            "Label accuracies statistics:\n",
            "{0: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 16: 1.0, 17: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 25: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 31: 1.0, 34: 1.0, 36: 1.0, 37: 1.0, 40: 1.0, 41: 1.0, 44: 1.0, 45: 1.0, 48: 1.0, 49: 1.0, 51: 1.0, 52: 1.0, 53: 1.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 1.0, 64: 1.0, 65: 0.6, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 75: 1.0, 76: 1.0, 77: 1.0, 78: 0.5, 79: 1.0, 80: 1.0, 81: 1.0, 82: 1.0, 83: 1.0, 84: 0.6, 85: 1.0, 86: 0.8, 87: 0.7, 88: 1.0, 89: 0.6, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 102: 1.0, 105: 1.0, 108: 1.0, 109: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 115: 1.0}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 3, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 5, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 6, 85: 5, 86: 4, 87: 7, 88: 5, 89: 3, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "{0: 5, 3: 5, 4: 5, 5: 5, 16: 5, 17: 5, 20: 5, 21: 5, 22: 5, 25: 5, 28: 5, 29: 5, 30: 5, 31: 5, 34: 5, 36: 5, 37: 5, 40: 5, 41: 5, 44: 4, 45: 5, 48: 5, 49: 5, 51: 5, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 5, 58: 5, 59: 10, 60: 10, 61: 5, 62: 5, 63: 10, 64: 5, 65: 5, 66: 10, 67: 5, 68: 5, 69: 10, 70: 5, 71: 5, 72: 5, 75: 10, 76: 5, 77: 5, 78: 10, 79: 5, 80: 5, 81: 10, 82: 5, 83: 5, 84: 10, 85: 5, 86: 5, 87: 10, 88: 5, 89: 5, 90: 5, 91: 5, 92: 5, 93: 10, 94: 5, 95: 5, 96: 10, 97: 5, 98: 5, 99: 10, 100: 5, 102: 4, 105: 5, 108: 5, 109: 5, 112: 5, 113: 5, 114: 5, 115: 5}\n",
            "\n",
            "checkpoint_v_2  ->  0.9648033126293996\n",
            "\n",
            "The top result was recorded at 0.9648033126293996 testing accuracy. The best checkpoint is lsa_64_spoter/checkpoint_t_1.\n",
            "\n",
            "Any desired statistics have been plotted.\n",
            "The experiment is finished.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wbRA3pLUpuXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etb8v_yHpdHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_process = process('/content/video_1.avi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDnavhm-9sQ4",
        "outputId": "c128c177-6db3-43f0-e73e-a550cdeec3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_results = pad_or_trim_landmarks(normalize_landmarks(x_process))"
      ],
      "metadata": {
        "id": "zZVdxJ4N-5iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = torch.tensor(normalized_results, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "edbUMRYz_H3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RsuHYRg_LIY",
        "outputId": "943e0dfb-25d5-4d9a-f127-6d5576fd5ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([200, 110])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "else:\n",
        "        device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "SMewZHdgABqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slrt_model = SPOTER(num_classes=400, hidden_dim=110)\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/hand_sigh_dataset/ckpt/ckpt_110/checkpoint_100_2_nopretrain.pth\", map_location=torch.device('cpu'))\n",
        "full_state_dict = checkpoint.state_dict()\n",
        "#new_state_dict = {k: v for k, v in full_state_dict.items() if 'linear_class' not in k}\n",
        "slrt_model.load_state_dict(full_state_dict, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwpPkLG5_Rmg",
        "outputId": "18cd18ee-1975-4d47-d06b-77b8f2956743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "<ipython-input-16-8774fe3a162a>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"/content/drive/MyDrive/hand_sigh_dataset/ckpt/ckpt_110/checkpoint_100_2_nopretrain.pth\", map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slrt_model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAOIP4P4AG5K",
        "outputId": "75dab460-03c5-4111-8e0b-fa27cb06a0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SPOTER(\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=110, out_features=110, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=110, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=110, bias=True)\n",
              "          (norm1): LayerNorm((110,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((110,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((110,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x SPOTERTransformerDecoderLayer(\n",
              "          (self_attn): DummySelfAttention()\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=110, out_features=110, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=110, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=110, bias=True)\n",
              "          (norm1): LayerNorm((110,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((110,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((110,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((110,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (linear_class): Linear(in_features=110, out_features=400, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = results.squeeze(0).to(device)"
      ],
      "metadata": {
        "id": "fpdumT5d_tio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnSfHEREvHeZ",
        "outputId": "ba865d4b-7d36-4274-e3b9-ef8b26d3f054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([200, 110])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = slrt_model(inputs).expand(1, -1, -1)"
      ],
      "metadata": {
        "id": "qbbfUUT9AKno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(int(torch.argmax(torch.nn.functional.softmax(outputs, dim=2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRfeC5uPAVdQ",
        "outputId": "528d930b-0dd2-460e-9514-e00b645aa8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set = HDF5Dataset(\"/content/drive/MyDrive/hand_sigh_dataset/h5/test_data_250.h5\")\n"
      ],
      "metadata": {
        "id": "De3gNXqBrXTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator()\n",
        "eval_loader = DataLoader(eval_set, shuffle=True, generator=g, pin_memory=True if torch.cuda.is_available() else False)"
      ],
      "metadata": {
        "id": "Bl6xV99RrY_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " _, _, eval_acc = evaluate(slrt_model, eval_loader, device, print_stats=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFTj-EbMr27_",
        "outputId": "193d16a6-99b4-4e20-aac9-e0ba8b6ed9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label accuracies statistics:\n",
            "{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 31: 1.0, 32: 1.0, 33: 1.0, 34: 1.0, 35: 1.0, 36: 1.0, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 1.0, 42: 1.0, 43: 1.0, 44: 1.0, 45: 1.0, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 1.0, 51: 1.0, 52: 1.0, 53: 1.0, 54: 1.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 1.0, 64: 1.0, 65: 1.0, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 73: 1.0, 74: 1.0, 75: 1.0, 76: 1.0, 77: 1.0, 78: 1.0, 79: 1.0, 80: 1.0, 81: 1.0, 82: 1.0, 83: 1.0, 84: 1.0, 85: 1.0, 86: 1.0, 87: 1.0, 88: 1.0, 89: 1.0, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 1.0, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 1.0, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 1.0, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 1.0, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 1.0, 180: 1.0, 181: 1.0, 182: 1.0, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 1.0, 191: 1.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 1.0, 200: 1.0, 201: 1.0, 202: 1.0, 203: 1.0, 204: 1.0, 205: 1.0, 206: 1.0, 207: 1.0, 208: 1.0, 209: 1.0, 210: 1.0, 211: 1.0, 212: 1.0, 213: 1.0, 214: 1.0, 215: 1.0, 216: 1.0, 217: 1.0, 218: 1.0, 219: 1.0, 220: 1.0, 221: 1.0, 222: 1.0, 223: 1.0, 224: 1.0, 225: 1.0, 226: 1.0, 227: 1.0, 228: 1.0, 229: 1.0, 230: 1.0, 231: 1.0, 232: 1.0, 233: 1.0, 234: 1.0, 235: 1.0, 236: 1.0, 237: 1.0, 238: 1.0, 239: 1.0, 240: 1.0, 241: 1.0, 242: 1.0, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0}\n",
            "\n",
            "{0: 3, 1: 1, 2: 1, 3: 1, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 1, 21: 1, 22: 1, 23: 3, 24: 1, 25: 1, 26: 1, 27: 3, 28: 1, 29: 3, 30: 1, 31: 3, 32: 1, 33: 3, 34: 1, 35: 1, 36: 1, 37: 1, 38: 3, 39: 1, 40: 3, 41: 3, 42: 3, 43: 3, 44: 3, 45: 3, 46: 3, 47: 3, 48: 3, 49: 3, 50: 3, 51: 3, 52: 3, 53: 3, 54: 3, 55: 3, 56: 1, 57: 3, 58: 1, 59: 1, 60: 1, 61: 1, 62: 3, 63: 3, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 3, 70: 1, 71: 1, 72: 1, 73: 1, 74: 3, 75: 1, 76: 1, 77: 3, 78: 1, 79: 3, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1, 100: 3, 101: 1, 102: 1, 103: 1, 104: 3, 105: 3, 106: 1, 107: 3, 108: 1, 109: 1, 110: 1, 111: 1, 112: 1, 113: 1, 114: 1, 115: 3, 116: 3, 117: 1, 118: 3, 119: 3, 120: 1, 121: 1, 122: 1, 123: 1, 124: 6, 125: 3, 126: 1, 127: 1, 128: 1, 129: 1, 130: 1, 131: 1, 132: 1, 133: 1, 134: 1, 135: 1, 136: 1, 137: 1, 138: 1, 139: 3, 140: 3, 141: 1, 142: 1, 143: 1, 144: 1, 145: 1, 146: 1, 147: 3, 148: 3, 149: 3, 150: 3, 151: 3, 152: 4, 153: 3, 154: 1, 155: 2, 156: 1, 157: 1, 158: 1, 159: 3, 160: 1, 161: 3, 162: 1, 163: 2, 164: 3, 165: 3, 166: 3, 167: 1, 168: 1, 169: 1, 170: 1, 171: 1, 172: 1, 173: 1, 174: 1, 175: 1, 176: 1, 177: 1, 178: 3, 179: 1, 180: 1, 181: 1, 182: 1, 183: 1, 184: 3, 185: 3, 186: 3, 187: 1, 188: 3, 189: 1, 190: 1, 191: 1, 192: 1, 193: 1, 194: 1, 195: 1, 196: 1, 197: 1, 198: 1, 199: 1, 200: 1, 201: 3, 202: 1, 203: 1, 204: 3, 205: 3, 206: 1, 207: 1, 208: 3, 209: 3, 210: 1, 211: 1, 212: 1, 213: 1, 214: 1, 215: 1, 216: 1, 217: 1, 218: 3, 219: 1, 220: 1, 221: 1, 222: 1, 223: 1, 224: 1, 225: 1, 226: 1, 227: 1, 228: 1, 229: 1, 230: 1, 231: 1, 232: 1, 233: 1, 234: 1, 235: 1, 236: 1, 237: 3, 238: 3, 239: 1, 240: 1, 241: 1, 242: 1, 243: 1, 244: 3, 245: 3, 246: 1, 247: 1, 248: 1, 249: 1}\n",
            "\n",
            "{0: 3, 1: 1, 2: 1, 3: 1, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 1, 21: 1, 22: 1, 23: 3, 24: 1, 25: 1, 26: 1, 27: 3, 28: 1, 29: 3, 30: 1, 31: 3, 32: 1, 33: 3, 34: 1, 35: 1, 36: 1, 37: 1, 38: 3, 39: 1, 40: 3, 41: 3, 42: 3, 43: 3, 44: 3, 45: 3, 46: 3, 47: 3, 48: 3, 49: 3, 50: 3, 51: 3, 52: 3, 53: 3, 54: 3, 55: 3, 56: 1, 57: 3, 58: 1, 59: 1, 60: 1, 61: 1, 62: 3, 63: 3, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 3, 70: 1, 71: 1, 72: 1, 73: 1, 74: 3, 75: 1, 76: 1, 77: 3, 78: 1, 79: 3, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1, 100: 3, 101: 1, 102: 1, 103: 1, 104: 3, 105: 3, 106: 1, 107: 3, 108: 1, 109: 1, 110: 1, 111: 1, 112: 1, 113: 1, 114: 1, 115: 3, 116: 3, 117: 1, 118: 3, 119: 3, 120: 1, 121: 1, 122: 1, 123: 1, 124: 6, 125: 3, 126: 1, 127: 1, 128: 1, 129: 1, 130: 1, 131: 1, 132: 1, 133: 1, 134: 1, 135: 1, 136: 1, 137: 1, 138: 1, 139: 3, 140: 3, 141: 1, 142: 1, 143: 1, 144: 1, 145: 1, 146: 1, 147: 3, 148: 3, 149: 3, 150: 3, 151: 3, 152: 4, 153: 3, 154: 1, 155: 2, 156: 1, 157: 1, 158: 1, 159: 3, 160: 1, 161: 3, 162: 1, 163: 2, 164: 3, 165: 3, 166: 3, 167: 1, 168: 1, 169: 1, 170: 1, 171: 1, 172: 1, 173: 1, 174: 1, 175: 1, 176: 1, 177: 1, 178: 3, 179: 1, 180: 1, 181: 1, 182: 1, 183: 1, 184: 3, 185: 3, 186: 3, 187: 1, 188: 3, 189: 1, 190: 1, 191: 1, 192: 1, 193: 1, 194: 1, 195: 1, 196: 1, 197: 1, 198: 1, 199: 1, 200: 1, 201: 3, 202: 1, 203: 1, 204: 3, 205: 3, 206: 1, 207: 1, 208: 3, 209: 3, 210: 1, 211: 1, 212: 1, 213: 1, 214: 1, 215: 1, 216: 1, 217: 1, 218: 3, 219: 1, 220: 1, 221: 1, 222: 1, 223: 1, 224: 1, 225: 1, 226: 1, 227: 1, 228: 1, 229: 1, 230: 1, 231: 1, 232: 1, 233: 1, 234: 1, 235: 1, 236: 1, 237: 3, 238: 3, 239: 1, 240: 1, 241: 1, 242: 1, 243: 1, 244: 3, 245: 3, 246: 1, 247: 1, 248: 1, 249: 1}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}